{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training with +ve Xeff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Input, Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = \"out_positive_xeff\"\n",
    "# import data\n",
    "data = pd.read_pickle(f\"{outdir}/p_a1_positive_xeff.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q</th>\n",
       "      <th>xeff</th>\n",
       "      <th>a1</th>\n",
       "      <th>p_a1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[0.012024048096192383, 0.014003959823454523, 0...</td>\n",
       "      <td>[0.009216418726310445, 0.028458441503637077, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>[0.1222444889779559, 0.12400351805816039, 0.12...</td>\n",
       "      <td>[0.0013008215791223796, 0.004041149522956928, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>[0.23246492985971942, 0.23400307629286626, 0.2...</td>\n",
       "      <td>[0.0010697856954987733, 0.0024382824268816272,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>[0.3426853707414829, 0.3440026345275721, 0.345...</td>\n",
       "      <td>[0.0009629691381983512, 0.00222891654232283, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[0.45290581162324645, 0.454002192762278, 0.455...</td>\n",
       "      <td>[0.001315515963205737, 0.0027549382395791115, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[0.002004008016032064, 0.004003999983935807, 0...</td>\n",
       "      <td>[0.004842086019793161, 0.00951858515585301, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>[0.20240480961923846, 0.2040031967743101, 0.20...</td>\n",
       "      <td>[0.0010434286955042496, 0.0013208399318507351,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>[0.40280561122244485, 0.4040023935646844, 0.40...</td>\n",
       "      <td>[0.0018007422164230593, 0.0023384387900644075,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>[0.6032064128256512, 0.6040015903550587, 0.604...</td>\n",
       "      <td>[0.0032252396366210193, 0.004504994041532742, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>[0.8036072144288576, 0.8040007871454331, 0.804...</td>\n",
       "      <td>[0.027112218815555603, 0.03243686299175742, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      q  xeff                                                 a1  \\\n",
       "0   0.1   0.1  [0.012024048096192383, 0.014003959823454523, 0...   \n",
       "1   0.1   0.2  [0.1222444889779559, 0.12400351805816039, 0.12...   \n",
       "2   0.1   0.3  [0.23246492985971942, 0.23400307629286626, 0.2...   \n",
       "3   0.1   0.4  [0.3426853707414829, 0.3440026345275721, 0.345...   \n",
       "4   0.1   0.5  [0.45290581162324645, 0.454002192762278, 0.455...   \n",
       "..  ...   ...                                                ...   \n",
       "85  1.0   0.5  [0.002004008016032064, 0.004003999983935807, 0...   \n",
       "86  1.0   0.6  [0.20240480961923846, 0.2040031967743101, 0.20...   \n",
       "87  1.0   0.7  [0.40280561122244485, 0.4040023935646844, 0.40...   \n",
       "88  1.0   0.8  [0.6032064128256512, 0.6040015903550587, 0.604...   \n",
       "89  1.0   0.9  [0.8036072144288576, 0.8040007871454331, 0.804...   \n",
       "\n",
       "                                                 p_a1  \n",
       "0   [0.009216418726310445, 0.028458441503637077, 0...  \n",
       "1   [0.0013008215791223796, 0.004041149522956928, ...  \n",
       "2   [0.0010697856954987733, 0.0024382824268816272,...  \n",
       "3   [0.0009629691381983512, 0.00222891654232283, 0...  \n",
       "4   [0.001315515963205737, 0.0027549382395791115, ...  \n",
       "..                                                ...  \n",
       "85  [0.004842086019793161, 0.00951858515585301, 0....  \n",
       "86  [0.0010434286955042496, 0.0013208399318507351,...  \n",
       "87  [0.0018007422164230593, 0.0023384387900644075,...  \n",
       "88  [0.0032252396366210193, 0.004504994041532742, ...  \n",
       "89  [0.027112218815555603, 0.03243686299175742, 0....  \n",
       "\n",
       "[90 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some data contains nans, drop them so that they wont cause problems later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.drop([0,1,2,3,4,5,6,7,8,9,10,11,21,22,32,33,43,44,54,55,65,66,76,77,87,88,98,99,109,110,120], inplace=True)\n",
    "# data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stupid way of getting everything to  numpy arrays with dtyoe = float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # a1\n",
    "# a1 = []\n",
    "# for i in range(len(data)):\n",
    "#     a1.append(data['a1'][i])\n",
    "# a1 = np.array(a1)\n",
    "# # p\n",
    "# p = []\n",
    "# for i in range(len(data)):\n",
    "#     p.append(data['p_a1'][i])\n",
    "# p = np.array(p)\n",
    "# # q\n",
    "# q = []\n",
    "# for i in data['q'].values:\n",
    "#     for j in range(len(a1[0])):\n",
    "#         q.append(i)\n",
    "# q = np.array(q)\n",
    "# # xeff \n",
    "# xeff = []\n",
    "# for i in data['xeff'].values:\n",
    "#     for j in range(len(a1[0])):\n",
    "#         xeff.append(i)\n",
    "# xeff = np.array(xeff)\n",
    "\n",
    "# a1 = a1.flatten()\n",
    "# p = p.flatten()\n",
    "\n",
    "# print(xeff.shape, q.shape, p.shape, a1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_q, test_q, train_xeff, test_xeff, train_p, test_p, train_a1, test_a1 \\\n",
    "= train_test_split(q, xeff, p, a1,\n",
    " test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_q_xeff =  np.stack((train_q, train_xeff), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define evoluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eva_model(model):\n",
    "    # evaluate the model\n",
    "    global test_a1\n",
    "    global test_p\n",
    "    scores = model.evaluate([test_q,test_xeff, test_a1], test_p)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    # plot history\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "    #\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.xlabel('epoches')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title('model loss')\n",
    "    plt.show()\n",
    "    #\n",
    "    # test_p_pred = model.predict([test_q,test_xeff, test_a1])\n",
    "    # # test_p_pred = scaler_p.inverse_transform(test_p_pred)\n",
    "    # # test_p = scaler_p.inverse_transform(test_p)\n",
    "    # # test_a1 = scaler_a.inverse_transform(test_a1)\n",
    "    # plt.plot(test_a1[3],test_p_pred[3],label = 'pred',color = 'red')\n",
    "    # plt.plot(test_a1[3],test_p[3],label = 'true',color = 'blue')\n",
    "    # plt.title(f'prediction vs true p_a1 for q = {test_q[3]}, xeff = {test_xeff[3]}')\n",
    "    # plt.legend()\n",
    "    # plt.xlabel('a1')\n",
    "    # plt.ylabel('p')\n",
    "    # plt.show()\n",
    "\n",
    "    # plt.plot(test_a1[8],test_p_pred[8],label = 'pred',color = 'red')\n",
    "    # plt.plot(test_a1[8],test_p[8],label = 'true',color = 'blue')\n",
    "    # plt.title(f'prediction vs true p_a1 for q = {test_q[5]}, xeff = {test_xeff[5]}')\n",
    "    # plt.legend()\n",
    "    # plt.xlabel('a1')\n",
    "    # plt.ylabel('p')\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "900/900 - 1s - loss: 5.3131 - accuracy: 0.0000e+00 - val_loss: 2.7768 - val_accuracy: 0.0000e+00 - 1s/epoch - 2ms/step\n",
      "Epoch 2/5000\n",
      "900/900 - 1s - loss: 2.4679 - accuracy: 0.0000e+00 - val_loss: 1.2859 - val_accuracy: 0.0000e+00 - 841ms/epoch - 934us/step\n",
      "Epoch 3/5000\n",
      "900/900 - 1s - loss: 1.8240 - accuracy: 0.0000e+00 - val_loss: 0.9451 - val_accuracy: 0.0000e+00 - 771ms/epoch - 857us/step\n",
      "Epoch 4/5000\n",
      "900/900 - 1s - loss: 1.4230 - accuracy: 0.0000e+00 - val_loss: 0.7147 - val_accuracy: 0.0000e+00 - 849ms/epoch - 943us/step\n",
      "Epoch 5/5000\n",
      "900/900 - 1s - loss: 1.2982 - accuracy: 0.0000e+00 - val_loss: 0.7095 - val_accuracy: 0.0000e+00 - 711ms/epoch - 790us/step\n",
      "Epoch 6/5000\n",
      "900/900 - 1s - loss: 1.2267 - accuracy: 0.0000e+00 - val_loss: 0.4819 - val_accuracy: 0.0000e+00 - 703ms/epoch - 781us/step\n",
      "Epoch 7/5000\n",
      "900/900 - 1s - loss: 1.1460 - accuracy: 0.0000e+00 - val_loss: 0.4297 - val_accuracy: 0.0000e+00 - 729ms/epoch - 810us/step\n",
      "Epoch 8/5000\n",
      "900/900 - 1s - loss: 1.0836 - accuracy: 0.0000e+00 - val_loss: 0.5123 - val_accuracy: 0.0000e+00 - 761ms/epoch - 845us/step\n",
      "Epoch 9/5000\n",
      "900/900 - 1s - loss: 1.1021 - accuracy: 0.0000e+00 - val_loss: 0.4215 - val_accuracy: 0.0000e+00 - 712ms/epoch - 791us/step\n",
      "Epoch 10/5000\n",
      "900/900 - 1s - loss: 1.0566 - accuracy: 0.0000e+00 - val_loss: 0.4585 - val_accuracy: 0.0000e+00 - 695ms/epoch - 773us/step\n",
      "Epoch 11/5000\n",
      "900/900 - 1s - loss: 1.0948 - accuracy: 0.0000e+00 - val_loss: 0.3253 - val_accuracy: 0.0000e+00 - 777ms/epoch - 863us/step\n",
      "Epoch 12/5000\n",
      "900/900 - 1s - loss: 1.1132 - accuracy: 0.0000e+00 - val_loss: 0.3776 - val_accuracy: 0.0000e+00 - 740ms/epoch - 822us/step\n",
      "Epoch 13/5000\n",
      "900/900 - 1s - loss: 1.0941 - accuracy: 0.0000e+00 - val_loss: 0.3201 - val_accuracy: 0.0000e+00 - 691ms/epoch - 768us/step\n",
      "Epoch 14/5000\n",
      "900/900 - 1s - loss: 1.1013 - accuracy: 0.0000e+00 - val_loss: 0.3782 - val_accuracy: 0.0000e+00 - 696ms/epoch - 773us/step\n",
      "Epoch 15/5000\n",
      "900/900 - 1s - loss: 1.0398 - accuracy: 0.0000e+00 - val_loss: 0.4621 - val_accuracy: 0.0000e+00 - 706ms/epoch - 784us/step\n",
      "Epoch 16/5000\n",
      "900/900 - 1s - loss: 1.0739 - accuracy: 0.0000e+00 - val_loss: 0.3198 - val_accuracy: 0.0000e+00 - 785ms/epoch - 872us/step\n",
      "Epoch 17/5000\n",
      "900/900 - 1s - loss: 1.0743 - accuracy: 0.0000e+00 - val_loss: 0.4562 - val_accuracy: 0.0000e+00 - 696ms/epoch - 773us/step\n",
      "Epoch 18/5000\n",
      "900/900 - 1s - loss: 1.0435 - accuracy: 0.0000e+00 - val_loss: 0.7001 - val_accuracy: 0.0000e+00 - 697ms/epoch - 775us/step\n",
      "Epoch 19/5000\n",
      "900/900 - 1s - loss: 1.0819 - accuracy: 0.0000e+00 - val_loss: 0.4561 - val_accuracy: 0.0000e+00 - 699ms/epoch - 776us/step\n",
      "Epoch 20/5000\n",
      "900/900 - 1s - loss: 1.0527 - accuracy: 0.0000e+00 - val_loss: 0.4711 - val_accuracy: 0.0000e+00 - 698ms/epoch - 776us/step\n",
      "Epoch 21/5000\n",
      "900/900 - 1s - loss: 1.0164 - accuracy: 0.0000e+00 - val_loss: 0.3525 - val_accuracy: 0.0000e+00 - 694ms/epoch - 771us/step\n",
      "Epoch 22/5000\n",
      "900/900 - 1s - loss: 1.0497 - accuracy: 0.0000e+00 - val_loss: 0.2829 - val_accuracy: 0.0000e+00 - 694ms/epoch - 771us/step\n",
      "Epoch 23/5000\n",
      "900/900 - 1s - loss: 1.0468 - accuracy: 0.0000e+00 - val_loss: 0.3837 - val_accuracy: 0.0000e+00 - 716ms/epoch - 796us/step\n",
      "Epoch 24/5000\n",
      "900/900 - 1s - loss: 1.1023 - accuracy: 0.0000e+00 - val_loss: 0.3626 - val_accuracy: 0.0000e+00 - 694ms/epoch - 771us/step\n",
      "Epoch 25/5000\n",
      "900/900 - 1s - loss: 1.0745 - accuracy: 0.0000e+00 - val_loss: 0.3391 - val_accuracy: 0.0000e+00 - 698ms/epoch - 776us/step\n",
      "Epoch 26/5000\n",
      "900/900 - 1s - loss: 1.0837 - accuracy: 0.0000e+00 - val_loss: 0.4253 - val_accuracy: 0.0000e+00 - 695ms/epoch - 772us/step\n",
      "Epoch 27/5000\n",
      "900/900 - 1s - loss: 1.0477 - accuracy: 0.0000e+00 - val_loss: 0.3981 - val_accuracy: 0.0000e+00 - 691ms/epoch - 768us/step\n",
      "Epoch 28/5000\n",
      "900/900 - 1s - loss: 1.1209 - accuracy: 0.0000e+00 - val_loss: 0.3525 - val_accuracy: 0.0000e+00 - 691ms/epoch - 767us/step\n",
      "Epoch 29/5000\n",
      "900/900 - 1s - loss: 1.1189 - accuracy: 0.0000e+00 - val_loss: 0.3736 - val_accuracy: 0.0000e+00 - 696ms/epoch - 773us/step\n",
      "Epoch 30/5000\n",
      "900/900 - 1s - loss: 1.0785 - accuracy: 0.0000e+00 - val_loss: 0.2819 - val_accuracy: 0.0000e+00 - 708ms/epoch - 787us/step\n",
      "Epoch 31/5000\n",
      "900/900 - 1s - loss: 1.0736 - accuracy: 0.0000e+00 - val_loss: 0.3211 - val_accuracy: 0.0000e+00 - 688ms/epoch - 765us/step\n",
      "Epoch 32/5000\n",
      "900/900 - 1s - loss: 1.0353 - accuracy: 0.0000e+00 - val_loss: 0.3721 - val_accuracy: 0.0000e+00 - 699ms/epoch - 777us/step\n",
      "Epoch 33/5000\n",
      "900/900 - 1s - loss: 1.0828 - accuracy: 0.0000e+00 - val_loss: 0.2816 - val_accuracy: 0.0000e+00 - 702ms/epoch - 780us/step\n",
      "Epoch 34/5000\n",
      "900/900 - 1s - loss: 1.0963 - accuracy: 0.0000e+00 - val_loss: 0.4211 - val_accuracy: 0.0000e+00 - 695ms/epoch - 772us/step\n",
      "Epoch 35/5000\n",
      "900/900 - 1s - loss: 1.1621 - accuracy: 0.0000e+00 - val_loss: 0.4180 - val_accuracy: 0.0000e+00 - 697ms/epoch - 775us/step\n",
      "Epoch 36/5000\n",
      "900/900 - 1s - loss: 1.1506 - accuracy: 0.0000e+00 - val_loss: 0.5632 - val_accuracy: 0.0000e+00 - 692ms/epoch - 769us/step\n",
      "Epoch 37/5000\n",
      "900/900 - 1s - loss: 1.1427 - accuracy: 0.0000e+00 - val_loss: 0.5375 - val_accuracy: 0.0000e+00 - 718ms/epoch - 797us/step\n",
      "Epoch 38/5000\n",
      "900/900 - 1s - loss: 1.1172 - accuracy: 0.0000e+00 - val_loss: 0.4135 - val_accuracy: 0.0000e+00 - 700ms/epoch - 778us/step\n",
      "Epoch 39/5000\n",
      "900/900 - 1s - loss: 1.1331 - accuracy: 0.0000e+00 - val_loss: 0.3536 - val_accuracy: 0.0000e+00 - 694ms/epoch - 771us/step\n",
      "Epoch 40/5000\n",
      "900/900 - 1s - loss: 1.1443 - accuracy: 0.0000e+00 - val_loss: 0.4966 - val_accuracy: 0.0000e+00 - 722ms/epoch - 802us/step\n",
      "Epoch 41/5000\n",
      "900/900 - 1s - loss: 1.1727 - accuracy: 0.0000e+00 - val_loss: 0.4796 - val_accuracy: 0.0000e+00 - 701ms/epoch - 779us/step\n",
      "Epoch 42/5000\n",
      "900/900 - 1s - loss: 1.0910 - accuracy: 0.0000e+00 - val_loss: 0.3208 - val_accuracy: 0.0000e+00 - 708ms/epoch - 787us/step\n",
      "Epoch 43/5000\n",
      "900/900 - 1s - loss: 1.1494 - accuracy: 0.0000e+00 - val_loss: 0.4234 - val_accuracy: 0.0000e+00 - 692ms/epoch - 769us/step\n",
      "Epoch 44/5000\n",
      "900/900 - 1s - loss: 1.1025 - accuracy: 0.0000e+00 - val_loss: 0.3181 - val_accuracy: 0.0000e+00 - 709ms/epoch - 788us/step\n",
      "Epoch 45/5000\n",
      "900/900 - 1s - loss: 1.1243 - accuracy: 0.0000e+00 - val_loss: 0.6505 - val_accuracy: 0.0000e+00 - 690ms/epoch - 767us/step\n",
      "Epoch 46/5000\n",
      "900/900 - 1s - loss: 1.1283 - accuracy: 0.0000e+00 - val_loss: 0.3911 - val_accuracy: 0.0000e+00 - 767ms/epoch - 852us/step\n",
      "Epoch 47/5000\n",
      "900/900 - 1s - loss: 1.1314 - accuracy: 0.0000e+00 - val_loss: 0.2778 - val_accuracy: 0.0000e+00 - 702ms/epoch - 780us/step\n",
      "Epoch 48/5000\n",
      "900/900 - 1s - loss: 1.1176 - accuracy: 0.0000e+00 - val_loss: 0.4378 - val_accuracy: 0.0000e+00 - 693ms/epoch - 770us/step\n",
      "Epoch 49/5000\n",
      "900/900 - 1s - loss: 1.1316 - accuracy: 0.0000e+00 - val_loss: 1.6453 - val_accuracy: 0.0000e+00 - 697ms/epoch - 774us/step\n",
      "Epoch 50/5000\n",
      "900/900 - 1s - loss: 1.1415 - accuracy: 0.0000e+00 - val_loss: 0.3151 - val_accuracy: 0.0000e+00 - 692ms/epoch - 769us/step\n",
      "Epoch 51/5000\n",
      "900/900 - 1s - loss: 1.1456 - accuracy: 0.0000e+00 - val_loss: 0.6509 - val_accuracy: 0.0000e+00 - 707ms/epoch - 785us/step\n",
      "Epoch 52/5000\n",
      "900/900 - 1s - loss: 1.0860 - accuracy: 0.0000e+00 - val_loss: 0.3396 - val_accuracy: 0.0000e+00 - 693ms/epoch - 770us/step\n",
      "Epoch 53/5000\n",
      "900/900 - 1s - loss: 1.1089 - accuracy: 0.0000e+00 - val_loss: 0.3852 - val_accuracy: 0.0000e+00 - 694ms/epoch - 771us/step\n",
      "Epoch 54/5000\n",
      "900/900 - 1s - loss: 1.0608 - accuracy: 0.0000e+00 - val_loss: 0.4415 - val_accuracy: 0.0000e+00 - 702ms/epoch - 780us/step\n",
      "Epoch 55/5000\n",
      "900/900 - 1s - loss: 1.1235 - accuracy: 0.0000e+00 - val_loss: 0.3791 - val_accuracy: 0.0000e+00 - 693ms/epoch - 770us/step\n",
      "Epoch 56/5000\n",
      "900/900 - 1s - loss: 1.0987 - accuracy: 0.0000e+00 - val_loss: 0.3633 - val_accuracy: 0.0000e+00 - 693ms/epoch - 770us/step\n",
      "Epoch 57/5000\n",
      "900/900 - 1s - loss: 1.1534 - accuracy: 0.0000e+00 - val_loss: 0.3196 - val_accuracy: 0.0000e+00 - 695ms/epoch - 773us/step\n",
      "Epoch 58/5000\n",
      "900/900 - 1s - loss: 1.0336 - accuracy: 0.0000e+00 - val_loss: 0.2851 - val_accuracy: 0.0000e+00 - 708ms/epoch - 787us/step\n",
      "Epoch 59/5000\n",
      "900/900 - 1s - loss: 1.1199 - accuracy: 0.0000e+00 - val_loss: 0.2878 - val_accuracy: 0.0000e+00 - 694ms/epoch - 772us/step\n",
      "Epoch 60/5000\n",
      "900/900 - 1s - loss: 1.1253 - accuracy: 0.0000e+00 - val_loss: 0.3081 - val_accuracy: 0.0000e+00 - 770ms/epoch - 856us/step\n",
      "Epoch 61/5000\n",
      "900/900 - 1s - loss: 1.1148 - accuracy: 0.0000e+00 - val_loss: 0.3633 - val_accuracy: 0.0000e+00 - 696ms/epoch - 773us/step\n",
      "Epoch 62/5000\n",
      "900/900 - 1s - loss: 1.1088 - accuracy: 0.0000e+00 - val_loss: 0.3717 - val_accuracy: 0.0000e+00 - 699ms/epoch - 776us/step\n",
      "Epoch 63/5000\n",
      "900/900 - 1s - loss: 1.1589 - accuracy: 0.0000e+00 - val_loss: 0.3283 - val_accuracy: 0.0000e+00 - 695ms/epoch - 772us/step\n",
      "Epoch 64/5000\n",
      "900/900 - 1s - loss: 1.0719 - accuracy: 0.0000e+00 - val_loss: 0.3364 - val_accuracy: 0.0000e+00 - 698ms/epoch - 776us/step\n",
      "Epoch 65/5000\n",
      "900/900 - 1s - loss: 1.1165 - accuracy: 0.0000e+00 - val_loss: 0.3299 - val_accuracy: 0.0000e+00 - 714ms/epoch - 793us/step\n",
      "Epoch 66/5000\n",
      "900/900 - 1s - loss: 1.0855 - accuracy: 0.0000e+00 - val_loss: 0.3303 - val_accuracy: 0.0000e+00 - 721ms/epoch - 801us/step\n",
      "Epoch 67/5000\n",
      "900/900 - 1s - loss: 1.0906 - accuracy: 0.0000e+00 - val_loss: 0.3167 - val_accuracy: 0.0000e+00 - 831ms/epoch - 924us/step\n",
      "Epoch 68/5000\n",
      "900/900 - 1s - loss: 1.0953 - accuracy: 0.0000e+00 - val_loss: 0.3270 - val_accuracy: 0.0000e+00 - 832ms/epoch - 925us/step\n",
      "Epoch 69/5000\n",
      "900/900 - 1s - loss: 1.0858 - accuracy: 0.0000e+00 - val_loss: 0.2960 - val_accuracy: 0.0000e+00 - 702ms/epoch - 781us/step\n",
      "Epoch 70/5000\n",
      "900/900 - 1s - loss: 1.0896 - accuracy: 0.0000e+00 - val_loss: 0.3931 - val_accuracy: 0.0000e+00 - 713ms/epoch - 793us/step\n",
      "Epoch 71/5000\n",
      "900/900 - 1s - loss: 1.0866 - accuracy: 0.0000e+00 - val_loss: 0.4084 - val_accuracy: 0.0000e+00 - 768ms/epoch - 854us/step\n",
      "Epoch 72/5000\n",
      "900/900 - 1s - loss: 1.1042 - accuracy: 0.0000e+00 - val_loss: 0.3439 - val_accuracy: 0.0000e+00 - 810ms/epoch - 900us/step\n",
      "Epoch 73/5000\n",
      "900/900 - 1s - loss: 1.0919 - accuracy: 0.0000e+00 - val_loss: 0.2796 - val_accuracy: 0.0000e+00 - 704ms/epoch - 782us/step\n",
      "Epoch 74/5000\n",
      "900/900 - 1s - loss: 1.1308 - accuracy: 0.0000e+00 - val_loss: 0.6053 - val_accuracy: 0.0000e+00 - 702ms/epoch - 781us/step\n",
      "Epoch 75/5000\n",
      "900/900 - 1s - loss: 1.1198 - accuracy: 0.0000e+00 - val_loss: 0.3575 - val_accuracy: 0.0000e+00 - 768ms/epoch - 853us/step\n",
      "Epoch 76/5000\n",
      "900/900 - 1s - loss: 1.0871 - accuracy: 0.0000e+00 - val_loss: 0.2962 - val_accuracy: 0.0000e+00 - 801ms/epoch - 890us/step\n",
      "Epoch 77/5000\n",
      "900/900 - 1s - loss: 1.0726 - accuracy: 0.0000e+00 - val_loss: 0.3115 - val_accuracy: 0.0000e+00 - 801ms/epoch - 890us/step\n",
      "Epoch 78/5000\n",
      "900/900 - 1s - loss: 1.0711 - accuracy: 0.0000e+00 - val_loss: 0.3204 - val_accuracy: 0.0000e+00 - 867ms/epoch - 963us/step\n",
      "Epoch 79/5000\n",
      "900/900 - 1s - loss: 1.1392 - accuracy: 0.0000e+00 - val_loss: 0.3695 - val_accuracy: 0.0000e+00 - 769ms/epoch - 854us/step\n",
      "Epoch 80/5000\n",
      "900/900 - 1s - loss: 1.0678 - accuracy: 0.0000e+00 - val_loss: 0.2699 - val_accuracy: 0.0000e+00 - 771ms/epoch - 856us/step\n",
      "Epoch 81/5000\n",
      "900/900 - 1s - loss: 1.0572 - accuracy: 0.0000e+00 - val_loss: 0.3500 - val_accuracy: 0.0000e+00 - 775ms/epoch - 861us/step\n",
      "Epoch 82/5000\n",
      "900/900 - 1s - loss: 1.0888 - accuracy: 0.0000e+00 - val_loss: 0.3660 - val_accuracy: 0.0000e+00 - 848ms/epoch - 942us/step\n",
      "Epoch 83/5000\n",
      "900/900 - 1s - loss: 1.0657 - accuracy: 0.0000e+00 - val_loss: 0.3828 - val_accuracy: 0.0000e+00 - 807ms/epoch - 896us/step\n",
      "Epoch 84/5000\n",
      "900/900 - 1s - loss: 1.0980 - accuracy: 0.0000e+00 - val_loss: 0.2877 - val_accuracy: 0.0000e+00 - 795ms/epoch - 883us/step\n",
      "Epoch 85/5000\n",
      "900/900 - 1s - loss: 1.0671 - accuracy: 0.0000e+00 - val_loss: 0.2963 - val_accuracy: 0.0000e+00 - 796ms/epoch - 885us/step\n",
      "Epoch 86/5000\n",
      "900/900 - 1s - loss: 1.0348 - accuracy: 0.0000e+00 - val_loss: 0.4203 - val_accuracy: 0.0000e+00 - 819ms/epoch - 910us/step\n",
      "Epoch 87/5000\n",
      "900/900 - 1s - loss: 1.0916 - accuracy: 0.0000e+00 - val_loss: 0.3319 - val_accuracy: 0.0000e+00 - 818ms/epoch - 909us/step\n",
      "Epoch 88/5000\n",
      "900/900 - 1s - loss: 1.1181 - accuracy: 0.0000e+00 - val_loss: 0.3638 - val_accuracy: 0.0000e+00 - 825ms/epoch - 917us/step\n",
      "Epoch 89/5000\n",
      "900/900 - 1s - loss: 1.1147 - accuracy: 0.0000e+00 - val_loss: 0.3342 - val_accuracy: 0.0000e+00 - 856ms/epoch - 951us/step\n",
      "Epoch 90/5000\n",
      "900/900 - 1s - loss: 1.1326 - accuracy: 0.0000e+00 - val_loss: 0.3539 - val_accuracy: 0.0000e+00 - 877ms/epoch - 974us/step\n",
      "Epoch 91/5000\n",
      "900/900 - 1s - loss: 1.0802 - accuracy: 0.0000e+00 - val_loss: 0.6674 - val_accuracy: 0.0000e+00 - 822ms/epoch - 913us/step\n",
      "Epoch 92/5000\n",
      "900/900 - 1s - loss: 1.0977 - accuracy: 0.0000e+00 - val_loss: 0.3637 - val_accuracy: 0.0000e+00 - 742ms/epoch - 825us/step\n",
      "Epoch 93/5000\n",
      "900/900 - 1s - loss: 1.1155 - accuracy: 0.0000e+00 - val_loss: 0.2734 - val_accuracy: 0.0000e+00 - 748ms/epoch - 831us/step\n",
      "Epoch 94/5000\n",
      "900/900 - 1s - loss: 1.1075 - accuracy: 0.0000e+00 - val_loss: 0.2667 - val_accuracy: 0.0000e+00 - 727ms/epoch - 808us/step\n",
      "Epoch 95/5000\n",
      "900/900 - 1s - loss: 1.0944 - accuracy: 0.0000e+00 - val_loss: 0.3467 - val_accuracy: 0.0000e+00 - 715ms/epoch - 795us/step\n",
      "Epoch 96/5000\n",
      "900/900 - 1s - loss: 1.0704 - accuracy: 0.0000e+00 - val_loss: 0.3457 - val_accuracy: 0.0000e+00 - 729ms/epoch - 811us/step\n",
      "Epoch 97/5000\n",
      "900/900 - 1s - loss: 1.1057 - accuracy: 0.0000e+00 - val_loss: 0.3715 - val_accuracy: 0.0000e+00 - 737ms/epoch - 819us/step\n",
      "Epoch 98/5000\n",
      "900/900 - 1s - loss: 1.0970 - accuracy: 0.0000e+00 - val_loss: 0.4532 - val_accuracy: 0.0000e+00 - 718ms/epoch - 798us/step\n",
      "Epoch 99/5000\n",
      "900/900 - 1s - loss: 1.1212 - accuracy: 0.0000e+00 - val_loss: 0.3448 - val_accuracy: 0.0000e+00 - 718ms/epoch - 798us/step\n",
      "Epoch 100/5000\n",
      "900/900 - 2s - loss: 1.0707 - accuracy: 0.0000e+00 - val_loss: 0.5476 - val_accuracy: 0.0000e+00 - 2s/epoch - 2ms/step\n",
      "Epoch 101/5000\n",
      "900/900 - 1s - loss: 1.0916 - accuracy: 0.0000e+00 - val_loss: 0.2949 - val_accuracy: 0.0000e+00 - 1s/epoch - 1ms/step\n",
      "Epoch 102/5000\n",
      "900/900 - 1s - loss: 1.0958 - accuracy: 0.0000e+00 - val_loss: 0.4022 - val_accuracy: 0.0000e+00 - 973ms/epoch - 1ms/step\n",
      "Epoch 103/5000\n",
      "900/900 - 1s - loss: 1.1193 - accuracy: 0.0000e+00 - val_loss: 0.4436 - val_accuracy: 0.0000e+00 - 792ms/epoch - 881us/step\n",
      "Epoch 104/5000\n",
      "900/900 - 1s - loss: 1.1663 - accuracy: 0.0000e+00 - val_loss: 0.3915 - val_accuracy: 0.0000e+00 - 762ms/epoch - 847us/step\n",
      "Epoch 105/5000\n",
      "900/900 - 1s - loss: 1.0838 - accuracy: 0.0000e+00 - val_loss: 0.3254 - val_accuracy: 0.0000e+00 - 770ms/epoch - 856us/step\n",
      "Epoch 106/5000\n",
      "900/900 - 1s - loss: 1.0949 - accuracy: 0.0000e+00 - val_loss: 0.3326 - val_accuracy: 0.0000e+00 - 738ms/epoch - 820us/step\n",
      "Epoch 107/5000\n",
      "900/900 - 1s - loss: 1.1091 - accuracy: 0.0000e+00 - val_loss: 0.3253 - val_accuracy: 0.0000e+00 - 752ms/epoch - 836us/step\n",
      "Epoch 108/5000\n",
      "900/900 - 1s - loss: 1.0970 - accuracy: 0.0000e+00 - val_loss: 0.2985 - val_accuracy: 0.0000e+00 - 733ms/epoch - 815us/step\n",
      "Epoch 109/5000\n",
      "900/900 - 1s - loss: 1.0882 - accuracy: 0.0000e+00 - val_loss: 0.2959 - val_accuracy: 0.0000e+00 - 714ms/epoch - 793us/step\n",
      "Epoch 110/5000\n",
      "900/900 - 1s - loss: 1.1329 - accuracy: 0.0000e+00 - val_loss: 0.3688 - val_accuracy: 0.0000e+00 - 726ms/epoch - 807us/step\n",
      "Epoch 111/5000\n",
      "900/900 - 1s - loss: 1.0993 - accuracy: 0.0000e+00 - val_loss: 0.2361 - val_accuracy: 0.0000e+00 - 742ms/epoch - 825us/step\n",
      "Epoch 112/5000\n",
      "900/900 - 1s - loss: 1.0747 - accuracy: 0.0000e+00 - val_loss: 0.3287 - val_accuracy: 0.0000e+00 - 777ms/epoch - 863us/step\n",
      "Epoch 113/5000\n",
      "900/900 - 1s - loss: 1.0728 - accuracy: 0.0000e+00 - val_loss: 0.3183 - val_accuracy: 0.0000e+00 - 844ms/epoch - 938us/step\n",
      "Epoch 114/5000\n",
      "900/900 - 1s - loss: 1.0968 - accuracy: 0.0000e+00 - val_loss: 0.2894 - val_accuracy: 0.0000e+00 - 783ms/epoch - 869us/step\n",
      "Epoch 115/5000\n",
      "900/900 - 1s - loss: 1.1381 - accuracy: 0.0000e+00 - val_loss: 0.2918 - val_accuracy: 0.0000e+00 - 770ms/epoch - 856us/step\n",
      "Epoch 116/5000\n",
      "900/900 - 1s - loss: 1.1116 - accuracy: 0.0000e+00 - val_loss: 0.2785 - val_accuracy: 0.0000e+00 - 718ms/epoch - 797us/step\n",
      "Epoch 117/5000\n",
      "900/900 - 1s - loss: 1.1329 - accuracy: 0.0000e+00 - val_loss: 0.3261 - val_accuracy: 0.0000e+00 - 750ms/epoch - 833us/step\n",
      "Epoch 118/5000\n",
      "900/900 - 1s - loss: 1.0813 - accuracy: 0.0000e+00 - val_loss: 0.2431 - val_accuracy: 0.0000e+00 - 798ms/epoch - 887us/step\n",
      "Epoch 119/5000\n",
      "900/900 - 1s - loss: 1.0516 - accuracy: 0.0000e+00 - val_loss: 0.3672 - val_accuracy: 0.0000e+00 - 816ms/epoch - 907us/step\n",
      "Epoch 120/5000\n",
      "900/900 - 1s - loss: 1.0867 - accuracy: 0.0000e+00 - val_loss: 0.4945 - val_accuracy: 0.0000e+00 - 738ms/epoch - 820us/step\n",
      "Epoch 121/5000\n",
      "900/900 - 1s - loss: 1.0993 - accuracy: 0.0000e+00 - val_loss: 0.3525 - val_accuracy: 0.0000e+00 - 746ms/epoch - 829us/step\n",
      "Epoch 122/5000\n",
      "900/900 - 1s - loss: 1.0799 - accuracy: 0.0000e+00 - val_loss: 0.3047 - val_accuracy: 0.0000e+00 - 736ms/epoch - 818us/step\n",
      "Epoch 123/5000\n",
      "900/900 - 1s - loss: 1.0797 - accuracy: 0.0000e+00 - val_loss: 0.2740 - val_accuracy: 0.0000e+00 - 740ms/epoch - 822us/step\n",
      "Epoch 124/5000\n",
      "900/900 - 1s - loss: 1.0582 - accuracy: 0.0000e+00 - val_loss: 0.3296 - val_accuracy: 0.0000e+00 - 802ms/epoch - 891us/step\n",
      "Epoch 125/5000\n",
      "900/900 - 1s - loss: 1.0986 - accuracy: 0.0000e+00 - val_loss: 0.2814 - val_accuracy: 0.0000e+00 - 813ms/epoch - 904us/step\n",
      "Epoch 126/5000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-eec0c3770096>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 metrics= ['accuracy'])\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m history = model.fit([train_q,train_xeff,train_a1], train_p, validation_split=0.2,\n\u001b[0m\u001b[1;32m     21\u001b[0m                     epochs=5000, batch_size=32, shuffle=True, verbose=2)\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2954\u001b[0m       (graph_function,\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Input1 = keras.Input(shape=(1,)) # q\n",
    "Input2 = keras.Input(shape=(1,)) # xeff\n",
    "Input3 = keras.Input(shape=(1,)) # a1\n",
    "merged = keras.layers.concatenate([Input1, Input2,Input3]) # Merging Laryer\n",
    "# \n",
    "x = keras.layers.Dense(32, activation='relu',\n",
    " kernel_regularizer= keras.regularizers.L2(0.01))(merged) # Fully Connected Layer\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "#\n",
    "output = keras.layers.Dense(1)(x) # Output\n",
    "#\n",
    "model = Model(inputs=[Input1,Input2,Input3], outputs=output)\n",
    "#\n",
    "# opt = keras.optimizers.Adam(learning_rate = 3e-3)\n",
    "model.compile(optimizer= 'adam',\n",
    "                loss='mse',\n",
    "                metrics= ['accuracy'])\n",
    "#\n",
    "history = model.fit([train_q,train_xeff,train_a1], train_p, validation_split=0.2,\n",
    "                    epochs=5000, batch_size=32, shuffle=True, verbose=2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "541143602329a03f71f32a0a160c785fb8ee263553dc2658377783b6f80d17e9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
